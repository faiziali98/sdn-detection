{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "#constant declaration\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 5e-5\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avergae_packet_size</th>\n",
       "      <th>syn_packets</th>\n",
       "      <th>synack_packets</th>\n",
       "      <th>ack_packets</th>\n",
       "      <th>data_packets</th>\n",
       "      <th>total_bytes</th>\n",
       "      <th>flow_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.000073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002235</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002403</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003256</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17275</th>\n",
       "      <td>0.008239</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.002013</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.001044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17276</th>\n",
       "      <td>0.008051</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.000972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17277</th>\n",
       "      <td>0.005138</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.999984</td>\n",
       "      <td>0.000914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17278</th>\n",
       "      <td>0.009407</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>0.001260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17279</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.001872</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17280 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       avergae_packet_size  syn_packets  synack_packets  ack_packets  \\\n",
       "0                 0.003205     0.000000        0.000000     0.000959   \n",
       "1                 0.002235     0.000012        0.000012     0.001009   \n",
       "2                 0.002609     0.000019        0.000015     0.000997   \n",
       "3                 0.002403     0.000006        0.000009     0.000936   \n",
       "4                 0.003256     0.000004        0.000004     0.000921   \n",
       "...                    ...          ...             ...          ...   \n",
       "17275             0.008239     0.000112        0.000112     0.002200   \n",
       "17276             0.008051     0.000139        0.000139     0.002151   \n",
       "17277             0.005138     0.000162        0.000162     0.001990   \n",
       "17278             0.009407     0.000180        0.000180     0.002160   \n",
       "17279             0.000000     0.000091        0.000091     0.001826   \n",
       "\n",
       "       data_packets  total_bytes  flow_count  \n",
       "0          0.000619     0.999994    0.000073  \n",
       "1          0.000599     0.999997    0.000067  \n",
       "2          0.000685     0.999996    0.000103  \n",
       "3          0.000603     0.999996    0.000048  \n",
       "4          0.000605     0.999994    0.000055  \n",
       "...             ...          ...         ...  \n",
       "17275      0.002013     0.999961    0.001044  \n",
       "17276      0.001943     0.999963    0.000972  \n",
       "17277      0.001320     0.999984    0.000914  \n",
       "17278      0.002025     0.999951    0.001260  \n",
       "17279      0.001872     0.999997    0.000000  \n",
       "\n",
       "[17280 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For full dataset, run tran_feature_selection notebook before this one\n",
    "# For small subset of data, run tran_sample_preprocessing notebook beofore this one (ideal for testing model on CPU)\n",
    "\n",
    "%store -r benign_flows  \n",
    "%store -r mixed_flows\n",
    "%store -r features\n",
    "\n",
    "dim = len(features)\n",
    "\n",
    "hyperparam_description = f'LR={LEARNING_RATE}, BatchSize={BATCH_SIZE}, #Features={len(features)}'\n",
    "\n",
    "#TODO look into scalars vs normalizers --> https://datascience.stackexchange.com/questions/45900/when-to-use-standard-scaler-and-when-normalizer\n",
    "\n",
    "normalizer = preprocessing.Normalizer(norm=\"l2\")\n",
    "normalized_train = normalizer.fit_transform(benign_flows[features]) #axis?\n",
    "train_X = pd.DataFrame(normalized_train, columns = features)\n",
    "\n",
    "normalized_test = normalizer.transform(mixed_flows[features])\n",
    "test_X = pd.DataFrame(normalized_test, columns = features)\n",
    "test_y = mixed_flows.is_attack\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.2050e-03, 0.0000e+00, 0.0000e+00,  ..., 6.1915e-04, 9.9999e-01,\n",
       "         7.2841e-05],\n",
       "        [2.2348e-03, 1.2229e-05, 1.2229e-05,  ..., 5.9922e-04, 1.0000e+00,\n",
       "         6.7259e-05],\n",
       "        [2.6093e-03, 1.9019e-05, 1.5215e-05,  ..., 6.8467e-04, 1.0000e+00,\n",
       "         1.0270e-04],\n",
       "        ...,\n",
       "        [5.1384e-03, 1.6248e-04, 1.6248e-04,  ..., 1.3202e-03, 9.9998e-01,\n",
       "         9.1395e-04],\n",
       "        [9.4072e-03, 1.8004e-04, 1.8004e-04,  ..., 2.0255e-03, 9.9995e-01,\n",
       "         1.2603e-03],\n",
       "        [0.0000e+00, 9.1299e-05, 9.1299e-05,  ..., 1.8716e-03, 1.0000e+00,\n",
       "         0.0000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset loading\n",
    "train_tensor = torch.tensor(train_X.values.astype(np.float32))\n",
    "train_loader = torch.utils.data.DataLoader(train_tensor, batch_size = BATCH_SIZE, shuffle = True)\n",
    "train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=dim, out_features=int(dim/2))\n",
    "        self.enc2 = nn.Linear(in_features=int(dim/2), out_features=int(dim/4))\n",
    "        self.enc3 = nn.Linear(in_features=int(dim/4), out_features=int(dim/8))\n",
    "        \n",
    "        # decoder \n",
    "        self.dec1 = nn.Linear(in_features=int(dim/8), out_features=int(dim/4))\n",
    "        self.dec2 = nn.Linear(in_features=int(dim/4), out_features=int(dim/2))\n",
    "        self.dec3 = nn.Linear(in_features=int(dim/2), out_features=dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # relu alternative: torch.sigmoid\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "\n",
    "        x = F.relu(self.dec1(x))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = F.relu(self.dec3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = Autoencoder()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "#Training model\n",
    "\n",
    "# Alternative loss functions - BCEWithLogitsLoss(), BCELoss()\n",
    "loss_function = nn.MSELoss() # Switching from BCELoss \n",
    "\n",
    "train_loss = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        input_data = data.to(device=device)\n",
    "        optimizer.zero_grad()\n",
    "        output = net(input_data).to(device=device)                  # output is the reconstruced x \n",
    "        loss = loss_function(output, input_data).to(device=device)  # input_data should be the target variable\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    loss = running_loss / len(train_loader)\n",
    "    train_loss.append(loss)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch {} of {}, Train Loss: {:.5f}'.format(\n",
    "          epoch+1, NUM_EPOCHS, loss))\n",
    "print(\"Completed training with final loss {:.5f}\".format(train_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1,figsize=(15,10))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Reconstruction Loss\")\n",
    "ax.set_title(f'Deep Auto-encoder using {loss_function}\\n{hyperparam_description}')\n",
    "ax.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reconstruction loss for test partition (mixed flow data)\n",
    "test_loss = []\n",
    "net.eval()\n",
    "test_tensor = torch.tensor(test_X.values.astype(np.float32))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_X)):\n",
    "        input = test_tensor[i].to(device=device)\n",
    "        output = net(input).to(device=device)\n",
    "        loss = loss_function(output, input).to(device=device)\n",
    "        test_loss.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_true=test_y.astype(int), y_score=test_loss, pos_label=1)\n",
    "ranked_thresholds = sorted(list(zip(np.abs(tpr - fpr), thresholds, tpr, fpr)), key=lambda i: i[0], reverse=True)\n",
    "_, attack_threshold, threshold_tpr, threshold_fpr = ranked_thresholds[0]\n",
    "print(f\"Selected Attack Threshold: {attack_threshold}\")\n",
    "print(\"Theshold yields TPR: {:.4f}, FPR: {:.4f}\".format(threshold_tpr, threshold_fpr))\n",
    "\n",
    "auc = roc_auc_score(y_true=test_y.astype(int),  y_score=test_loss)\n",
    "print(\"AUC: {:.4f}\".format(auc))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\") # plot baseline curve\n",
    "plt.plot(fpr, tpr, marker=\".\", label=\"Attack Threshold:{:.6f}\\nTPR: {:.4f}, FPR:{:.4f}\".format(attack_threshold, threshold_tpr, threshold_fpr))\n",
    "plt.axhline(y=threshold_tpr, color='darkgreen', lw=0.8, ls='--')\n",
    "plt.axvline(x=threshold_fpr, color='darkgreen', lw=0.8, ls='--')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = test_y.to_frame().astype(bool)\n",
    "test_results['loss'] = pd.Series(test_loss, index=test_results.index)\n",
    "test_results['is_attack_prediction'] = test_results.loss > attack_threshold\n",
    "\n",
    "conf_matrix = confusion_matrix(test_results.is_attack, test_results.is_attack_prediction)\n",
    "plt.figure()\n",
    "sns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 16}, fmt='g')\n",
    "plt.title('Attack Threshold Classification - Confusion Matrix')\n",
    "print(classification_report(test_results.is_attack, test_results.is_attack_prediction, target_names=[\"benign\", \"attack\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_loss = test_results[test_results['is_attack'] == False].loss\n",
    "sns.displot(benign_loss, bins=100, kde=True, color='navy', height=8, aspect=2)\n",
    "plt.axvline(attack_threshold, 0.0, 10, color='darkgreen', linestyle='dashed')\n",
    "plt.title('Test Dataset - Benign Flow Loss Distribution')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_loss = test_results[test_results['is_attack'] == True].loss\n",
    "sns.displot(attack_loss, bins=100, kde=True, color='crimson', height=8, aspect=2)\n",
    "plt.axvline(attack_threshold, 0.0, 10, color='darkgreen', linestyle='dashed')\n",
    "plt.title('Test Dataset - Attack Flow Loss Distribution')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "attack_loss = test_results[test_results['is_attack'] == True].loss\n",
    "benign_loss = test_results[test_results['is_attack'] == False].loss\n",
    "sns.histplot(benign_loss, ax=ax, bins=100, kde=True, color='navy', alpha=0.5)\n",
    "sns.histplot(attack_loss, ax=ax, bins=100, kde=True, color='crimson', alpha=0.5)\n",
    "\n",
    "plt.axvline(attack_threshold, 0.0, 10, color='darkgreen', linestyle='dashed')\n",
    "plt.title('Test Dataset - Mixed Flow Loss Distribution')\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating loss for training-set on trained model (for toubleshooting distributions)\n",
    "post_train_loss = [] \n",
    "net.eval()\n",
    "train_tensor = torch.tensor(train_X.values.astype(np.float32))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(train_X)):\n",
    "        input = train_tensor[i].to(device=device)\n",
    "        output = net(input).to(device=device)\n",
    "        loss = loss_function(output, input).to(device=device)\n",
    "        post_train_loss.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph loss density for training set post training\n",
    "post_train_results = pd.DataFrame({'is_attack': [False] * len(post_train_loss)})\n",
    "post_train_results['loss'] = pd.Series(post_train_loss, index=post_train_results.index)\n",
    "post_train_results['is_attack_prediction'] = post_train_results.loss > attack_threshold\n",
    "\n",
    "sns.displot(post_train_results.loss, bins=100, kde=True, color='navy', height=8, aspect=2)\n",
    "plt.axvline(attack_threshold, 0.0, 10, color='darkgreen', linestyle='dashed')\n",
    "plt.title('Training Dataset - Post-training Benign Flow Loss Distribution')\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
